* Meta-data :noexport:
  # http://orgmode.org/worg/exporters/beamer/tutorial.html
  #+TITLE: What is the @@latex:\\@@
  #+TITLE: Expectation Maximization @@latex:\\@@
  #+TITLE: (EM) Algorithm? @@latex:\\@@
  #+AUTHOR: Kazuki Yoshida @@latex:\\@@
  #+AUTHOR: @@latex:\\@@
  #+AUTHOR: Division of Rheumatology, Immunology and Allergy @@latex:\\@@
  #+AUTHOR: Brigham and Women's Hospital & Harvard Medical School @@latex:\\@@
  #+AUTHOR: \faTwitter @kaz_yos \faGithub kaz-yos \faEnvelope kazukiyoshida@mail.harvard.edu
  #+DATE: 2019-05-20@@latex:\\@@
  #+DATE: Mini-Statistics Camp Series @@latex:\\@@
  #+DATE: BWH Bioinformatics Club @@latex:\\@@
  #+DESCRIPTION:
  #+KEYWORDS:
  #+OPTIONS: toc:nil
  #+OPTIONS: H:2
  #+OPTIONS: ^:{}
  #+STARTUP: beamer
  #+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
  #+LATEX_CLASS: beamer
  #+LATEX_CLASS_OPTIONS: [dvipdfmx,bigger,aspectratio=169]
  #+LATEX_HEADER: %% No navigation bar
  #+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
  #+LATEX_HEADER: %% Page number with current/total format
  #+LATEX_HEADER: \setbeamerfont{page number in head/foot}{size=\scriptsize}
  #+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
  #+LATEX_HEADER: \setbeamertemplate{frametitle}[default][center]
  #+LATEX_HEADER: \setbeamersize{text margin left=5mm,text margin right=5mm}
  #+LATEX_HEADER: %% With item labels
  #+LATEX_HEADER: \setbeamertemplate{bibliography item}{\insertbiblabel}
  #+LATEX_HEADER: %% Without item labels
  #+LATEX_HEADER: %% \setbeamertemplate{bibliography item}{}
  #+LATEX_HEADER:
  #+LATEX_HEADER: %% Math
  #+LATEX_HEADER: \usepackage{amsmath}
  #+LATEX_HEADER: \usepackage{amssymb}
  #+LATEX_HEADER: \usepackage{wasysym}
  #+LATEX_HEADER: %% Allow new page within align
  #+LATEX_HEADER: \allowdisplaybreaks
  #+LATEX_HEADER: \usepackage{cancel}
  #+LATEX_HEADER: %% Code
  #+LATEX_HEADER: \usepackage{listings}
  #+LATEX_HEADER: \usepackage{courier}
  #+LATEX_HEADER: \lstset{basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single}
  #+LATEX_HEADER: \usepackage[cache=false]{minted}
  #+LATEX_HEADER: \usemintedstyle{vs}
  #+LATEX_HEADER: %% Graphics
  #+LATEX_HEADER: \usepackage{graphicx}
  #+LATEX_HEADER: \usepackage{grffile}
  #+LATEX_HEADER: %% DAG
  #+LATEX_HEADER: \usepackage{tikz}
  #+LATEX_HEADER: \usetikzlibrary{positioning,shapes.geometric}
  #+LATEX_HEADER: %% Allow URL embedding
  #+LATEX_HEADER: \usepackage{url}
  #+LATEX_HEADER: %% Do not count backup slides.
  #+LATEX_HEADER: %% https://tex.stackexchange.com/questions/70448/dont-count-backup-slides
  #+LATEX_HEADER: \usepackage{appendixnumberbeamer}
  #+LATEX_HEADER: %% https://www.sharelatex.com/learn/Hyperlinks
  #+LATEX_HEADER: \usepackage{hyperref}
  #+LATEX_HEADER: \hypersetup{
  #+LATEX_HEADER:     colorlinks = true,
  #+LATEX_HEADER:     linkcolor= blue
  #+LATEX_HEADER: }
  #+LATEX_HEADER: \usepackage{fontawesome}
  #+LATEX_HEADER: %% Include convenient commands.
  #+LATEX_HEADER: \input{\string~/.emacs.d/misc/GrandMacros}

* Opening
** Personal Background
- 2005 /Igakushi/ degree (\approx MD) in Japan.
- -2012 Trained clinically as a rheumatologist/internist.
- 2014 Master of Public Health (MPH) at Harvard T.H. Chan School of Public Health and research fellowship at Brigham and Women's Hospital.


- 9/2018 Doctor of Science (ScD) in Epidemiology and Biostatistics (Committee: Drs. Sonia Hernandez-Diaz, Robert J Glynn, Daniel H Solomon, Sebastien Haneuse) at \includegraphics[height=0.7cm]{./source/HarvardChan_logo_hrz_alt_RGB_Large.png}
- 11/2018- Associate Epidemiologist/Instructor in Medicine at \includegraphics[height=0.7cm]{./source/bwh.png}


* Introduction
** Article Covered
- Do and Batzoglou. "What is the expectation maximization algorith?" Nat. Biotechnol. 2008;26:897. cite:doWhatExpectationMaximization2008
#+ATTR_LATEX: :width 0.8\textwidth :options page=1,keepaspectratio :center t
[[./source/em_algo.png]]
\footnotesize
Presented as a part of the Mini Statistics Camp 2019 hosted by the [[http://bioinformatics.bwh.harvard.edu][BWH/HMS Bioinformatics Club]]

** Introduction
- Probabilistic models, such as hidden Markov models and Bayesian networks, are commonly used to model biological data.


- Often, the only data available for training probabilistic models are incomplete, requiring special handling.


- The Expectation-Maximization (EM) algorithm enables parameter estimation in probabilistic models with incomplete data.

** Incomplete data
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.45
    :END:
- Incomplete data encompass:
  - Typical missing data
  - Latent class (entirely unobserved class assignment)

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.45
    :END:
#+ATTR_LATEX: :height 0.75\textheight :width \textwidth :options page=1,keepaspectratio :center t
[[./source/missing_patterns.png]]
\scriptsize cite:littleStatisticalAnalysisMissing2002 \normalsize

* Experiment Setup
** A Coin-Flipping Experiment
- Two coins $A$ (0) and $B$ (1) with unknown head probabilities $(\theta_{0},\theta_{1})$
- Repeat 5 times
  1. Randomly pick either coin with equal probability and record
  2. Toss 10 times and record the number of heads
#+ATTR_LATEX: :height 0.5\textheight :options page=1,keepaspectratio :center t
[[./source/experiment_data.png]]

** A Coin-Flipping Experiment (More Formal)
- Two coins $A$ (0) and $B$ (1) with unknown head probabilities $(\theta_{0},\theta_{1})$
- For $i = 1, \dots, 5$
  1. Draw $Z_{i} \sim \text{Bernoulli}(p = 0.5), Z_{i} \in \left\{ 0,1 \right\}$
  2. Draw $X_{i} | Z_{i} \sim \text{Binomial}(n = 10, p = \theta_{Z_{i}}), X_{i} \in \left\{ 0, \dots, 10 \right\}$
| Index |    Coin |   Heads |
|   $i$ | $Z_{i}$ | $X_{i}$ |
|-------+---------+---------|
|     1 |       1 |       5 |
|     2 |       0 |       9 |
|     3 |       0 |       8 |
|     4 |       1 |       4 |
|     5 |       0 |       7 |
- Note that you only need the number of heads ([[https://www.statisticshowto.datasciencecentral.com/sufficient-statistic/][sufficient statistic]]), not the entire sequence.

* Complete-Data Case
** Complete-Data Maximum Likelihood
- If we observe both the coin identity $Z_{i}$ and heads $X_{i}$, the MLE is the total heads / total tosses for each coin.
- Here we introduce a very redundant expanded table for later reuse.
\footnotesize
| Index | Coin    |                    Prob. Coin A |                Prob. Coin B |   Heads | Heads Coin A                           | Heads Coin B                       |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert Z_{i},X_{i}]$ | $E[Z_{i}\vert Z_{i},X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert Z_{i},X_{i}]$ | $E[Z_{i} X_{i} \vert Z_{i},X_{i}]$ |
|-------+---------+---------------------------------+-----------------------------+---------+----------------------------------------+------------------------------------|
|     / | <>      |                               < |                           > |         | <                                      | >                                  |
|     1 | 1 (B)   |                               0 |                           1 |       5 | 0 \times 5                             | 1 \times 5                         |
|     2 | 0 (A)   |                               1 |                           0 |       9 | 1 \times 9                             | 0 \times 9                         |
|     3 | 0 (A)   |                               1 |                           0 |       8 | 1 \times 8                             | 0 \times 8                         |
|     4 | 1 (B)   |                               0 |                           1 |       4 | 0 \times 4                             | 1 \times 4                         |
|     5 | 0 (A)   |                               1 |                           0 |       7 | 1 \times 7                             | 0 \times 7                         |
|-------+---------+---------------------------------+-----------------------------+---------+----------------------------------------+------------------------------------|
|   Sum |         |                               3 |                           2 |      33 | 24                                     | 9                                  |
- MLE: $\thetahat_{0} = 24 / (3 \times 10) = 0.80$; $\thetahat_{1} = 9 / (2 \times 10) = 0.45$

** Complete-Data Likelihood and Log Likelihood
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
\begin{align*}
  L(\btheta | \bz,\bx)
  &= \prod^{5}_{i=1} p(z_{i},x_{i} | \btheta)\\
  &= \prod^{5}_{i=1} p(x_{i} | z_{i}, \btheta) p(z_{i} | \btheta)\\
  &= \prod^{5}_{i=1} p(x_{i} | z_{i}, \btheta) p(z_{i})\\
  &= \prod^{5}_{i=1} p(x_{i} | z_{i}, \btheta) (0.5)\\
  &\propto \prod^{5}_{i=1}
    \left[ \theta_{0}^{x_{i}}(1-\theta_{0})^{10-x_{i}} \right]^{1-z_{i}}
    \left[ \theta_{1}^{x_{i}}(1-\theta_{1})^{10-x_{i}} \right]^{z_{i}}\\
  \log L(\btheta | \bz,\bx)
  &\propto \sum^{5}_{i=1}
    \left\{
    (1-z_{i}) \log \left[ \theta_{0}^{x_{i}}(1-\theta_{0})^{10-x_{i}} \right] +
    z_{i} \log \left[ \theta_{1}^{x_{i}}(1-\theta_{1})^{10-x_{i}} \right]
    \right\}\\
  &= \sum^{5}_{i=1}
    \left\{
    (1-z_{i}) \left[ x_{i}\log\theta_{0} + (10-x_{i})\log(1-\theta_{0})\right]
    \right.\\
  &~~~~~~~~~ \left.
    + z_{i} \left[ x_{i}\log\theta_{1} + (10-x_{i})\log(1-\theta_{1})\right]
    \right\}
\end{align*}
- We can take partial derivatives with respect to $\theta_{0}$ and $\theta_{1}$ and set them to zero to solve for MLE, which gives us heads/tosses for each coin.

** Complete-Data Likelihood Visualization
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- Here the complete-data likelihood function is convex.
- There is a unique maximum with an analytical solution (coin-specific heads/tosses).

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 7 :height 7
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/likelihood.pdf :exports results
suppressMessages(library(tidyverse)); options(crayon.enabled = FALSE)
construct_llik <- function(z, x) {
  stopifnot(length(z) == length(x))
  llik <- function(theta0, theta1) {
    stopifnot(length(theta0) == length(theta1))
    if (length(theta0) > 1) {
      return((Vectorize(llik))(theta0, theta1))
    }
    z0_contrib <- (x * log(theta0) + (10 - x) * log(1 - theta0))
    z1_contrib <- (x * log(theta1) + (10 - x) * log(1 - theta1))
    return(sum((1 - z) * z0_contrib) + sum(z * z1_contrib))
  }
}
llik <- construct_llik(z = c(1,0,0,1,0),
                       x = c(5,9,8,4,7))
inc <- 0.01
grid <- seq(from = inc, to = 1 - inc, by = inc)
lik_data <- crossing(theta0 = grid,
                     theta1 = grid) %>%
  mutate(llik = llik(theta0, theta1)) %>%
  mutate(llik_scaled = llik - max(llik, na.rm = TRUE)) %>%
  mutate(lik_scaled = exp(llik_scaled),
         lik = exp(llik))
lik_data %>%
  select(theta0, theta1, lik) %>%
  spread(key = theta1, value = lik) %>%
  as.matrix(.) %>% `[`(,-1) %>%
  persp(x = grid, y = grid, z = .,
        xlim = c(0,1), ylim = c(0,1),
        main = "Complete-Data Likelihood",
        xlab = "theta0", ylab = "theta1", zlab = "Likelihood")
#+END_SRC

#+ATTR_LATEX: :height \textheight :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/likelihood.pdf]]
\normalsize

* Experiment Setup (Incomplete Data)
** A Contrived Coin-Flipping Experiment
- Two identical-looking coins with unknown head probabilities
- Repeat 5 times
  1. You are randomly given either coin, but you do not know which.
  2. You toss 10 times, record the number of heads, and return the coin.
| Index | Coin    |   Heads |
|   $i$ | $Z_{i}$ | $X_{i}$ |
|-------+---------+---------|
|     1 | ?       |       5 |
|     2 | ?       |       9 |
|     3 | ?       |       8 |
|     4 | ?       |       4 |
|     5 | ?       |       7 |
- Can we still estimate the two unknown head probabilities given this incomplete data?

** A Contrived Coin-Flipping Experiment (More Formal)
- Two identical-looking coins with unknown head probabilities $(\theta_{0},\theta_{1})$ (index arbitrary)
- For $i = 1, \dots, 5$
  1. Draw /latent/ $Z_{i} \sim \text{Bernoulli}(p = 0.5), Z_{i} \in \left\{ 0,1 \right\}$
  2. Draw $X_{i} | Z_{i} \sim \text{Binomial}(n = 10, p = \theta_{Z_{i}}), X_{i} \in \left\{ 0, \dots, 10 \right\}$
| Index | Coin    |   Heads |
|   $i$ | $Z_{i}$ | $X_{i}$ |
|-------+---------+---------|
|     1 | ?       |       5 |
|     2 | ?       |       9 |
|     3 | ?       |       8 |
|     4 | ?       |       4 |
|     5 | ?       |       7 |

** How Do We Approach Incomplete Data
- Now we cannot compute the proportion of heads among tosses for each coin.
- However, one possible iterative scheme is:
  - Assign some initial guess for parameters
  - Guess coin identities given data and assuming these parameter values
  - Perform MLE given data and assuming coin identities
- The Expectation-Maximization (EM) Algorithm cite:dempsterMaximumLikelihoodIncomplete1977 is a refinement of this idea for MLE.
- The Data Augmentation Method cite:tannerCalculationPosteriorDistributions1987 is another type of refinement for Bayesian estimation.

* EM Algorithm
** Title EM
   :PROPERTIES:
   :BEAMER_ENV: fullframe
   :END:
#+BEGIN_CENTER
\resizebox{\linewidth}{!}{Expectation-Maximization Algorithm}
#+END_CENTER

** EM Algorithm to the Rescue
- The Expectation-Maximization (EM) Algorithm cite:dempsterMaximumLikelihoodIncomplete1977
- After random initialization of parameters, two steps alternates until convergence to an MLE.
- Steps repeated
  1. E-Step (compute Expected sufficient statistics):
     - Estimate probabilities of latent states given current parameters (coin identity probabilities)
     - Obtain expected sufficient statistics (weighted head counts distributed across coins)
  2. M-Step (Maximize expected log-likelihood):
     - Obtain MLE of parameters given expected sufficient statistics and update parameters
- By using weighted training data, the EM algorithm accounts for the confidence in the guessed latent state.

** EM Algorithm in One Figure
   :PROPERTIES:
   :BEAMER_ENV: fullframe
   :END:
#+ATTR_LATEX: :height \textheight :width \textwidth :options page=1,keepaspectratio :center t
[[./source/em_figure.png]]


** Parameter Initialization
- Randomly initialize the parameters
  - $\thetahat_{0}^{(0)} := 0.6$
  - $\thetahat_{1}^{(0)} := 0.5$

** E-Step (1)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Current parameters: $\thetahat_{0}^{(0)} = 0.6, \thetahat_{1}^{(0)} = 0.5$
\footnotesize
| Index | Coin    | Prob. Coin A              | Prob. Coin B          |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      | <                         | >                     |         | <                                | >                            |
|     1 | ?       | ?                         | ?                     |       5 | ? \times 5                       | ? \times 5                   |
|     2 | ?       | ?                         | ?                     |       9 | ? \times 9                       | ? \times 9                   |
|     3 | ?       | ?                         | ?                     |       8 | ? \times 8                       | ? \times 8                   |
|     4 | ?       | ?                         | ?                     |       4 | ? \times 4                       | ? \times 4                   |
|     5 | ?       | ?                         | ?                     |       7 | ? \times 7                       | ? \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         | ?                         | ?                     |      33 | ?                                | ?                            |
\normalsize
- First, we need coin probabilities for each $i$ given the current parameter values $\bthetahat^{(0)}$.
- We will focus on $E_{\bthetahat^{(0)}}[Z_{i} | X_{i} = x_{i}]$, the probability of Coin B given the number of heads observed and current parameters.

\newpage
- Probability of Coin B given the number of heads observed and current parameters:
\footnotesize
\begin{align*}
  E_{\bthetahat^{(0)}}[Z_{i} | X_{i} = x_{i}] &= P_{\bthetahat^{(0)}}[Z_{i} = 1 | X_{i} = x_{i}]\\
  &~~~\text{Bayes rule}\\
  &= \frac{P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1] P_{\bthetahat^{(0)}}[Z_{i} = 1]}
          {\sum\limits^{1}_{z=0} P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = z] P_{\bthetahat^{(0)}}[Z_{i} = z]}\\
  &~~~\text{Coin choice probability = 0.5}\\
  &= \frac{P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1] (0.5)}
          {\sum\limits^{1}_{z=0} P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = z] (0.5)}\\
  &= \frac{P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1]}
          {P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 0] + P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1]}
\end{align*}
\normalsize

\newpage
\footnotesize
\begin{align*}
  E_{\bthetahat^{(0)}}[Z_{i} | X_{i} = x_{i}]
  &= \frac{P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1]}
          {P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 0] + P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = 1]}
\end{align*}
\normalsize
- $P_{\bthetahat^{(0)}}[X_{i} = x_{i} | Z_{i} = z]$ is the probability mass (=dbinom=) of the observed $X_{i}$ assuming coin identity $z$ and current parameters.
- Thus, this quantity, the probability of Coin B given the the observed $X_{i}$ and the current parameters, can be calculated as follows for the first row (5 heads).
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
A <- dbinom(x = 5, size = 10, prob = 0.60) # Prob. of 5 heads given Coin A
B <- dbinom(x = 5, size = 10, prob = 0.50) # Prob. of 5 heads given Coin B
B / (A + B)                                # Prob. of Coin B given 5 heads
#+END_SRC
\normalsize

\newpage
- Now we have the probabilities of coin identities.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.45 |                  0.55 |       5 | ? \times 5                       | ? \times 5                   |
|     2 | ?       |                      0.80 |                  0.20 |       9 | ? \times 9                       | ? \times 9                   |
|     3 | ?       |                      0.73 |                  0.27 |       8 | ? \times 8                       | ? \times 8                   |
|     4 | ?       |                      0.35 |                  0.65 |       4 | ? \times 4                       | ? \times 4                   |
|     5 | ?       |                      0.65 |                  0.35 |       7 | ? \times 7                       | ? \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.99 |                  2.01 |         | ?                                | ?                            |
\normalsize

\newpage
- Now we weight the contribution of sufficient statistics accordingly.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.45 |                  0.55 |       5 | 0.45 \times 5                    | 0.55 \times 5                |
|     2 | ?       |                      0.80 |                  0.20 |       9 | 0.80 \times 9                    | 0.20 \times 9                |
|     3 | ?       |                      0.73 |                  0.27 |       8 | 0.73 \times 8                    | 0.27 \times 8                |
|     4 | ?       |                      0.35 |                  0.65 |       4 | 0.35 \times 4                    | 0.65 \times 4                |
|     5 | ?       |                      0.65 |                  0.35 |       7 | 0.65 \times 7                    | 0.35 \times 7                |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.99 |                  2.01 |         | ?                                | ?                            |
\normalsize

\newpage
- Calculate the expected heads and consider expected tosses.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.45 |                  0.55 |       5 | 0.45 \times 5                    | 0.55 \times 5                |
|     2 | ?       |                      0.80 |                  0.20 |       9 | 0.80 \times 9                    | 0.20 \times 9                |
|     3 | ?       |                      0.73 |                  0.27 |       8 | 0.73 \times 8                    | 0.27 \times 8                |
|     4 | ?       |                      0.35 |                  0.65 |       4 | 0.35 \times 4                    | 0.65 \times 4                |
|     5 | ?       |                      0.65 |                  0.35 |       7 | 0.65 \times 7                    | 0.35 \times 7                |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.99 |                  2.01 |         | 21.3                             | 11.7                         |
\normalsize
- In expectation, Coin A was chosen 2.99 times, resulting in 29.9 expected tosses, whereas Coin B was chosen 2.01 times, resulting in 20.1 expected tosses.
- The observed heads are distributed across coins. The sums indicate 21.3 expected heads for Coin A and 11.7 expected heads for Coin B.

** M-Step (1)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Now using the current expected heads and tosses for each coin, recalculate the MLE.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.45 |                  0.55 |       5 | 0.45 \times 5                    | 0.55 \times 5                |
|     2 | ?       |                      0.80 |                  0.20 |       9 | 0.80 \times 9                    | 0.20 \times 9                |
|     3 | ?       |                      0.73 |                  0.27 |       8 | 0.73 \times 8                    | 0.27 \times 8                |
|     4 | ?       |                      0.35 |                  0.65 |       4 | 0.35 \times 4                    | 0.65 \times 4                |
|     5 | ?       |                      0.65 |                  0.35 |       7 | 0.65 \times 7                    | 0.35 \times 7                |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.99 |                  2.01 |         | 21.3                             | 11.7                         |
\normalsize
- MLE: $\thetahat_{0}^{(1)} = 21.3 / (2.99 \times 10) = 0.71$; $\thetahat_{1}^{(1)} = 11.7 / (2.01 \times 10) = 0.58$

** E-Step (2)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Current parameters: $\thetahat_{0}^{(1)} = 0.71, \thetahat_{1}^{(1)} = 0.58$
- Calculate the probabilities again and update the expected tosses and heads.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.30 |                  0.70 |       5 | 0.30 \times 5                    | 0.70 \times 5                |
|     2 | ?       |                      0.81 |                  0.19 |       9 | 0.81 \times 9                    | 0.19 \times 9                |
|     3 | ?       |                      0.71 |                  0.29 |       8 | 0.71 \times 8                    | 0.29 \times 8                |
|     4 | ?       |                      0.19 |                  0.81 |       4 | 0.19 \times 4                    | 0.81 \times 4                |
|     5 | ?       |                      0.57 |                  0.43 |       7 | 0.57 \times 7                    | 0.43 \times 7                |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.58 |                  2.42 |      33 | 19.21                            | 13.79                        |
\normalsize
- In expectation, Coin A was chosen 2.58 times, resulting in 25.8 expected tosses, whereas Coin B was chosen 2.42 times, resulting in 24.2 expected tosses.
- The sums indicate 19.21 expected heads for Coin A and 13.79 expected heads for Coin B.

** M-Step (2)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Now using the current expected heads and tosses for each coin, recalculate the MLE.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.30 |                  0.70 |       5 | 0.30 \times 5                    | 0.70 \times 5                |
|     2 | ?       |                      0.81 |                  0.19 |       9 | 0.81 \times 9                    | 0.19 \times 9                |
|     3 | ?       |                      0.71 |                  0.29 |       8 | 0.71 \times 8                    | 0.29 \times 8                |
|     4 | ?       |                      0.19 |                  0.81 |       4 | 0.19 \times 4                    | 0.81 \times 4                |
|     5 | ?       |                      0.57 |                  0.43 |       7 | 0.57 \times 7                    | 0.43 \times 7                |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                      2.58 |                  2.42 |      33 | 19.21                            | 13.79                        |
\normalsize
- MLE: $\thetahat_{0}^{(2)} = 19.21 / (2.58 \times 10) = 0.75$; $\thetahat_{1}^{(2)} = 13.79 / (2.42 \times 10) = 0.57$

** Automated Version
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- The =em_step= function perform one cycle of the E-step and M-step.
\tiny
#+BEGIN_SRC R :session *R-org* :results output :exports both
suppressMessages(library(tidyverse)); options(crayon.enabled = FALSE)
rel_dbinom <- function(X, theta) {
  p_X_Z0 <- dbinom(x = X, size = 10, prob = theta[1])
  p_X_Z1 <- dbinom(x = X, size = 10, prob = theta[2])
  tibble("Prob. Coin A" = p_X_Z0 / (p_X_Z0 + p_X_Z1),
         "Prob. Coin B" = p_X_Z1 / (p_X_Z0 + p_X_Z1))
}
em_step <- function(theta) {
  X <- c(5,9,8,4,7)
  exp_choice <- bind_rows(rel_dbinom(X[1], theta),
                          rel_dbinom(X[2], theta),
                          rel_dbinom(X[3], theta),
                          rel_dbinom(X[4], theta),
                          rel_dbinom(X[5], theta))
  exp_head <- sweep(exp_choice, MARGIN = 1, STATS = X, FUN = "*")
  colnames(exp_head) <- c("Heads Coin A","Heads Coin B")
  E <- bind_cols(tibble(Index = c(as.character(1:5), "Sum")),
                 bind_rows(exp_choice, colSums(exp_choice)),
                 tibble(X = c(X, sum(X))),
                 bind_rows(exp_head, colSums(exp_head)))
  M <- as.numeric(colSums(exp_head) / (colSums(exp_choice) * 10))
  list(E = E, M = M)
}
#+END_SRC
\normalsize

** EM Step 3
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
em_step(theta = c(0.6, 0.5)) %>% magrittr::extract2("M") %>%
  em_step() %>% magrittr::extract2("M") %>%
  em_step()
#+END_SRC
\normalsize

** Iterative Version
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- The =em_iter= function fully automate the iterations until convergence at the specified tolerance.
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
em_iter <- function(theta, tolerance = 10^(-3)) {
  thetas <- tibble(theta0 = theta[1], theta1 = theta[2])
  theta_prev <- theta
  theta_curr <- em_step(theta)$M
  while (sqrt(sum((theta_curr - theta_prev)^2)) > tolerance) {
    theta_prev <- theta_curr
    thetas <- bind_rows(thetas, tibble(theta0 = theta_prev[1], theta1 = theta_prev[2]))
    theta_curr <- em_step(theta_prev)$M
  }
  thetas <- bind_rows(thetas, tibble(theta0 = theta_curr[1], theta1 = theta_curr[2]))
  return(thetas)
}
#+END_SRC
\normalsize

\newpage
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
em_iter(theta = c(0.6, 0.5), tolerance = 10^(-3))
#+END_SRC
\normalsize

** Visual Representation of Iteration
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- The algorithm deterministically converge to the local maximum by monotonically improving the parameter estimate.


- As with most optimization methods for non-concave function (i.e., multiple local maxima), the EM algorithm comes with guarantees only of convergence to a local maximum.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 5 :height 5
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/em_figure1.pdf :exports results
crossing(init0 = 0.60,
         init1 = 0.50) %>%
  mutate(data = map2(init0, init1, function(init0, init1) {
    em_iter(theta = c(init0, init1), tolerance = 10^(-3))
  })) %>%
  unnest() %>%
  ggplot(mapping = aes(x = theta0, y = theta1,
                       group = interaction(init0,init1))) +
  geom_path(size = 1, arrow = arrow(angle = 15, ends = "last", type = "closed")) +
  scale_x_continuous(limits = 0:1) +
  scale_y_continuous(limits = 0:1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5),
        legend.key = element_blank(),
        plot.title = element_text(hjust = 0.5),
        strip.background = element_blank())
#+END_SRC

#+ATTR_LATEX: :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/em_figure1.pdf]]
\normalsize


** Multiple Initialization and Label Indeterminancy
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- Multiple initial starting parameters are often helpful.
- In this instance, at least three $\bthetahat$ seem to exist: (0.80, 0.52), (0.52, 0.80), (0.66, 0.66).
- Note $\btheta = (0.80, 0.52)$ and $\btheta = (0.52, 0.80)$ give the same models because the labeling $\theta_{0}$ and $\theta_{1}$ (which coin we call 0 or 1) is arbitrary.
- $\btheta = (0.66, 0.66)$ corresponds to a model where we really only have one type of coins.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 5 :height 5
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/em_figure2.pdf :exports results
init <- seq(from = 0.05, to = 0.95, by = 0.05)
crossing(init0 = init,
         init1 = init) %>%
  mutate(data = map2(init0, init1, function(init0, init1) {
    em_iter(theta = c(init0, init1), tolerance = 10^(-3))
  })) %>%
  unnest() %>%
  ggplot(mapping = aes(x = theta0, y = theta1,
                       group = interaction(init0,init1))) +
  geom_path(size = 0.1) +
  scale_x_continuous(limits = 0:1) +
  scale_y_continuous(limits = 0:1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5),
        legend.key = element_blank(),
        plot.title = element_text(hjust = 0.5),
        strip.background = element_blank())
#+END_SRC

#+ATTR_LATEX: :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/em_figure2.pdf]]
\normalsize

** Incomplete-Data Likelihood
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- In this specific instance, the incomplete-data likelihood can be graphed with grid search as the parameter space is small and low dimensional ([0,1]^{2}).
- The incomplete-data likelihood is bimodal and has a saddle point between the modes.
- This shape explains the three solutions.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 7 :height 7
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/likelihood2.pdf :exports results
construct_llik2 <- function(x) {
  llik <- function(theta0, theta1) {
    stopifnot(length(theta0) == length(theta1))
    if (length(theta0) > 1) {
      return((Vectorize(llik))(theta0, theta1))
    }
    z0_contrib <- (x * log(theta0) + (10 - x) * log(1 - theta0))
    z1_contrib <- (x * log(theta1) + (10 - x) * log(1 - theta1))
    return(sum(log(`+`(exp(z0_contrib), exp(z1_contrib)))))
  }
}
llik2 <- construct_llik2(x = c(5,9,8,4,7))
inc <- 0.01
grid <- seq(from = inc, to = 1 - inc, by = inc)
lik_data <- crossing(theta0 = grid,
                     theta1 = grid) %>%
  mutate(llik2 = llik2(theta0, theta1)) %>%
  mutate(lik = exp(llik2))
lik_data %>%
  select(theta0, theta1, lik) %>%
  spread(key = theta1, value = lik) %>%
  as.matrix(.) %>% `[`(,-1) %>%
  persp(x = grid, y = grid, z = .,
        xlim = c(0,1), ylim = c(0,1),
        main = "Incomplete-Data Likelihood",
        xlab = "theta0", ylab = "theta1", zlab = "Likelihood")
#+END_SRC

** Monotone Improvement in EM Algorithm
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- This part proves that the EM Algorithm is guaranteed to improve the parameter estimate toward the local optimum every step. cite:doWhatExpectationMaximization2008,murphyMachineLearningProbabilistic2012
\footnotesize
\begin{align*}
  \log \left( p(\bx | \btheta) \right)
  &= \log \left( \sum_{\bz} p(\bx, \bz | \btheta) \right)\\
  &~~~\text{Introduce arbitrary distribution $Q$}\\
  &= \log \left( \sum_{\bz} Q(\bz) \frac{p(\bx, \bz | \btheta)}{Q(\bz)} \right)\\
  &~~~\text{Rewrite as expectation}\\
  &= \log \left( E_{Q} \left[ \frac{p(\bx, \bz | \btheta)}{Q(\bz)} \right] \right)\\
  &~~~\text{Jensen's inequality on concave log}\\
  &\ge E_{Q} \left[ \log \left( \frac{p(\bx, \bz | \btheta)}{Q(\bz)} \right) \right]\\
  &= \sum_{\bz} Q(\bz) \log \left( \frac{p(\bx, \bz | \btheta)}{Q(\bz)} \right)\\
  &= \sum_{\bz} Q(\bz) \log \left( \frac{p(\bz | \bx, \btheta) p(\bx | \btheta)}{Q(\bz)} \right)\\
  &= \sum_{\bz} Q(\bz) \log \left( \frac{p(\bz | \bx, \btheta)}{Q(\bz)} \right) + \sum_{\bz} Q(\bz) \log \left( p(\bx | \btheta) \right)\\
  &= \sum_{\bz} Q(\bz) \log \left( \frac{p(\bz | \bx, \btheta)}{Q(\bz)} \right) + \log \left( p(\bx | \btheta) \right) \sum_{\bz} Q(\bz)\\
  &= \log \left( p(\bx | \btheta) \right) + \sum_{\bz} Q(\bz) \log \left( \frac{p(\bz | \bx, \btheta)}{Q(\bz)} \right)\\
  &= \log \left( p(\bx | \btheta) \right) - \Kbb\Lbb \left( Q(\bz) || p(\bz | \bx, \btheta) \right)
\end{align*}
\normalsize
- This inequality gives the lower bound for $\log \left( p(\bx | \btheta) \right)$ for all $\btheta$.
- This lower bound is improved (maximized) by reducing the KL divergence cite:murphyMachineLearningProbabilistic2012 by setting $Q(\bz) = p(\bz | \bx, \btheta)$, which also gives equality.
- As $\btheta$ is the unknown quantity that we want to estimate, we can use $Q(\bz) = p(\bz | \bx, \bthetahat^{(t)})$ as our best available option. In this case, equality holds at $\log \left( p(\bx | \bthetahat^{(t)}) \right)$.
- Consider the following function $g_{t}(\btheta)$, which uses $Q(\bz) = p(\bz | \bx, \bthetahat^{(t)})$. Note that only the numerator term within the log has a free parameter $\btheta$.
\footnotesize
\begin{align*}
  g_{t}(\btheta) &= \sum_{\bz} p \left( \bz | \bx, \bthetahat^{(t)} \right) \log \left( \frac{p(\bx,\bz | \btheta)}{p \left( \bz | \bx, \bthetahat^{(t)} \right)} \right)
\end{align*}
\normalsize
- Note that $\log \left( p(\bx | \btheta) \right) \ge g_{t}(\btheta)$ for all $\btheta$ by the inequality.
- At $\bthetahat^{(t)}$, $g_{t}(\bthetahat^{(t)})$ meets the equality condition, thus, $g_{t}(\bthetahat^{(t)}) = \log p(\bx | \bthetahat^{(t)})$. That is, $g_{t}$ "touches" the incomplete-data likelihood function at the current parameter estimates. cite:murphyMachineLearningProbabilistic2012
- Consider an update rule to find $\btheta^{(t+1)}$ that maximizes this $g_{t}$ function: $\bthetahat^{(t+1)} = \arg\max_{\btheta} g_{t}(\btheta)$. Then the following inequality holds.
\footnotesize
\begin{align*}
  &~~~\text{By above inequality}\\
  \log p \left( \bx | \bthetahat^{(t+1)} \right)
  &\ge g_{t}\left( \bthetahat^{(t+1)} \right)\\
  &~~~\text{As $\bthetahat^{(t+1)}$ maximizes $g_{t}$}\\
  &\ge g_{t}\left( \bthetahat^{(t)} \right)\\
  &~~~\text{Equality holds at current value}\\
  &= \log p \left( \bx | \bthetahat^{(t)} \right)
\end{align*}
\normalsize
- Therefore, $\log p \left( \bx | \bthetahat^{(t+1)} \right) \ge \log p \left( \bx | \bthetahat^{(t)} \right)$. That is, this update rule is guaranteed to improve the parameter estimate for the incomplete-data likelihood at each step.
- Now compare this update rule to the EM algorithm.
\footnotesize
\begin{align*}
  \bthetahat^{(t+1)}
  &= \arg\max_{\btheta} g_{t}(\btheta)\\
  &= \arg\max_{\btheta} \sum_{\bz} p \left( \bz | \bx, \bthetahat^{(t)} \right) \log \left( \frac{p(\bx,\bz | \btheta)}{p \left( \bz | \bx, \bthetahat^{(t)} \right)} \right)\\
  &= \arg\max_{\btheta} \sum_{\bz} p \left( \bz | \bx, \bthetahat^{(t)} \right)
    \left[
    \log p(\bx,\bz | \btheta)
    -
    \log p \left( \bz | \bx, \bthetahat^{(t)} \right)
    \right]\\
  &~~~\text{Drop constant second term free of $\btheta$}\\
  &= \arg\max_{\btheta} \sum_{\bz} p \left( \bz | \bx, \bthetahat^{(t)} \right) \log p(\bx,\bz | \btheta)
\end{align*}
\normalsize
- This is maximization of the expected complete-data log likelihood. The expectation is over the distribution $\bz$ given the observed data $\bx$ and assuming the current parameter value $\bthetahat^{(t)}$.
- Therefore, the EM algorithm is equivalent to the update rule with the guaranteed improvement at each step.

* DA Method
** Title DA
   :PROPERTIES:
   :BEAMER_ENV: fullframe
   :END:
#+BEGIN_CENTER
\resizebox{\linewidth}{!}{Data Augmentation Method}
#+END_CENTER

** From EM to DA
- A related Bayesian computation method is the /Data Augmentation/ method. cite:tannerCalculationPosteriorDistributions1987,tannerEMDataAugmentation2010
- After random initialization of parameters, two steps alternates until convergence to a posterior distribution.
- Steps repeated
  1. Imputation (I) Step:
     - Estimate probabilities of latent states given current parameters
     - Draw a latent state
  2. Posterior (P) Step:
     - Draw new parameters given data and latent state

** Model Configuration
- To set up a Bayesian computation, we need probability models for the data (likelihood) as well as the parameters (prior).
- Likelihood
\begin{align*}
  Z_{i} &\sim \text{Bernoulli}(p = 0.5), Z_{i} \in \left\{ 0,1 \right\}\\
  X_{i} | Z_{i}, \btheta &\sim \text{Binomial}(n = 10, p = \theta_{Z_{i}}), X_{i} \in \left\{ 0, \dots, 10 \right\}
\end{align*}
- Prior
\begin{align*}
  \theta_{0} &\sim \text{Beta}(a_{0},b_{0})\\
  \theta_{1} &\sim \text{Beta}(a_{1},b_{1})
\end{align*}
- Here we will consider independent uniform priors ($a_{j} = b_{j} = 1, j = 0,1$).

** Parameter Initialization
- Randomly initialize the parameters
  - $\theta_{0}^{(0)} := 0.6$
  - $\theta_{1}^{(0)} := 0.5$

** I-Step (1)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Current parameters: $\theta_{0}^{(0)} = 0.6, \theta_{1}^{(0)} = 0.5$
\footnotesize
| Index | Coin    | Prob. Coin A              | Prob. Coin B          |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      | <                         | >                     |         | <                                | >                            |
|     1 | ?       | ?                         | ?                     |       5 | ? \times 5                       | ? \times 5                   |
|     2 | ?       | ?                         | ?                     |       9 | ? \times 9                       | ? \times 9                   |
|     3 | ?       | ?                         | ?                     |       8 | ? \times 8                       | ? \times 8                   |
|     4 | ?       | ?                         | ?                     |       4 | ? \times 4                       | ? \times 4                   |
|     5 | ?       | ?                         | ?                     |       7 | ? \times 7                       | ? \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | ?                                | ?                            |
\normalsize
- First, we need coin probabilities for each $i$ given the current parameter values $\btheta^{(0)}$.
- This calculation is the same as the EM algorithm.

\newpage
- Now we have the probabilities of coin identities.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | ?       |                      0.45 |                  0.55 |       5 | ? \times 5                       | ? \times 5                   |
|     2 | ?       |                      0.80 |                  0.20 |       9 | ? \times 9                       | ? \times 9                   |
|     3 | ?       |                      0.73 |                  0.27 |       8 | ? \times 8                       | ? \times 8                   |
|     4 | ?       |                      0.35 |                  0.65 |       4 | ? \times 4                       | ? \times 4                   |
|     5 | ?       |                      0.65 |                  0.35 |       7 | ? \times 7                       | ? \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | ?                                | ?                            |
\normalsize
- We will now draw $Z_{i}^{(1)}$.
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
set.seed(737265171)
rbinom(n = 5, size = 1, prob = c(0.55, 0.20, 0.27, 0.65, 0.35))
#+END_SRC
\normalsize

\newpage
- We have imputed the latent coin identities.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | 0       |                      0.45 |                  0.55 |       5 | ? \times 5                       | ? \times 5                   |
|     2 | 0       |                      0.80 |                  0.20 |       9 | ? \times 9                       | ? \times 9                   |
|     3 | 0       |                      0.73 |                  0.27 |       8 | ? \times 8                       | ? \times 8                   |
|     4 | 1       |                      0.35 |                  0.65 |       4 | ? \times 4                       | ? \times 4                   |
|     5 | 0       |                      0.65 |                  0.35 |       7 | ? \times 7                       | ? \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | ?                                | ?                            |
\normalsize
- We will proceed assuming these imputed latent coin identities.

\newpage
- We have imputed the latent coin identities.
\footnotesize
| Index |    Coin |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / |      <> |                         < |                     > |         | <                                | >                            |
|     1 |       0 |                      0.45 |                  0.55 |       5 | 1 \times 5                       | 0 \times 5                   |
|     2 |       0 |                      0.80 |                  0.20 |       9 | 1 \times 9                       | 0 \times 9                   |
|     3 |       0 |                      0.73 |                  0.27 |       8 | 1 \times 8                       | 0 \times 8                   |
|     4 |       1 |                      0.35 |                  0.65 |       4 | 0 \times 4                       | 1 \times 4                   |
|     5 |       0 |                      0.65 |                  0.35 |       7 | 1 \times 7                       | 0 \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | 29                               | 4                            |
\normalsize
- We will proceed assuming these imputed latent coin identities.


** P-Step (1)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Now using the complete data on $(\bZ,\bX)$, construct a posterior $p(\btheta | \bZ, \bX)$.
\footnotesize
| Index | Coin    |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / | <>      |                         < |                     > |         | <                                | >                            |
|     1 | 0       |                      0.45 |                  0.55 |       5 | 1 \times 5                       | 0 \times 5                   |
|     2 | 0       |                      0.80 |                  0.20 |       9 | 1 \times 9                       | 0 \times 9                   |
|     3 | 0       |                      0.73 |                  0.27 |       8 | 1 \times 8                       | 0 \times 8                   |
|     4 | 1       |                      0.35 |                  0.65 |       4 | 0 \times 4                       | 1 \times 4                   |
|     5 | 0       |                      0.65 |                  0.35 |       7 | 1 \times 7                       | 0 \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |         | 29                               | 4                            |
\normalsize
- Using imputed coin identities, we have 29 head and 11 tails (40 tosses) for Coin A and 4 heads and 6 tails (10 tosses) for Coin B.

\newpage
- By conjugacy, we can updated the beta distributions as follows.
\begin{align*}
  \theta_{0}^{(1)} &\sim \text{Beta}(1 + 29, 1 + 11)\\
  \theta_{1}^{(1)} &\sim \text{Beta}(1 + 4, 1 + 6)
\end{align*}
- Draw updated values.
\tiny
#+BEGIN_SRC R :session *R-org* :results output :exports both
c(rbeta(n = 1, shape1 = 1 + 29, shape2 = 1 + 11),
  rbeta(n = 1, shape1 = 1 +  4, shape2 = 1 +  6)) %>% round(3)
#+END_SRC
\normalsize
- We now have updated parameter draws: $\thetahat_{0}^{(1)} := 0.760$; $\thetahat_{1}^{(1)} := 0.471$

** I-Step (2)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- Current parameters: $\theta_{0}^{(1)} = 0.760, \theta_{1}^{(1)} = 0.471$
- Calculate the probabilities again and impute the latent states.
\footnotesize
| Index |    Coin |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / |      <> |                         < |                     > |         | <                                | >                            |
|     1 |       1 |                      0.17 |                  0.83 |       5 | 0 \times 5                       | 1 \times 5                   |
|     2 |       0 |                      0.97 |                  0.03 |       9 | 1 \times 9                       | 0 \times 9                   |
|     3 |       0 |                      0.90 |                  0.10 |       8 | 1 \times 8                       | 0 \times 8                   |
|     4 |       1 |                      0.06 |                  0.94 |       4 | 0 \times 4                       | 1 \times 4                   |
|     5 |       0 |                      0.73 |                  0.27 |       7 | 1 \times 7                       | 0 \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | 24                               | 9                            |
\normalsize
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
rbinom(n = 5, size = 1, prob = c(0.83, 0.03, 0.10, 0.94, 0.27))
#+END_SRC
\normalsize
\newpage
- Using imputed coin identities, we have 24 head and 6 tails (30 tosses) for Coin A and 9 heads and 11 tail (20 tosses) for Coin B.
\footnotesize
| Index |    Coin |              Prob. Coin A |          Prob. Coin B |   Heads | Heads Coin A                     | Heads Coin B                 |
|   $i$ | $Z_{i}$ | $E[(1-Z_{i})\vert X_{i}]$ | $E[Z_{i}\vert X_{i}]$ | $X_{i}$ | $E[(1-Z_{i}) X_{i} \vert X_{i}]$ | $E[Z_{i} X_{i} \vert X_{i}]$ |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|     / |      <> |                         < |                     > |         | <                                | >                            |
|     1 |       1 |                      0.17 |                  0.83 |       5 | 0 \times 5                       | 1 \times 5                   |
|     2 |       0 |                      0.97 |                  0.03 |       9 | 1 \times 9                       | 0 \times 9                   |
|     3 |       0 |                      0.90 |                  0.10 |       8 | 1 \times 8                       | 0 \times 8                   |
|     4 |       1 |                      0.06 |                  0.94 |       4 | 0 \times 4                       | 1 \times 4                   |
|     5 |       0 |                      0.73 |                  0.27 |       7 | 1 \times 7                       | 0 \times 7                   |
|-------+---------+---------------------------+-----------------------+---------+----------------------------------+------------------------------|
|   Sum |         |                           |                       |      33 | 24                               | 9                            |
\normalsize

** P-Step (2)
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- We have 24 head and 6 tails (30 tosses) for Coin A and 9 heads and 11 tail (20 tosses) for Coin B.
- The posterior distributions are:
\begin{align*}
  \theta_{0}^{(2)} &\sim \text{Beta}(1 + 24, 1 + 6)\\
  \theta_{1}^{(2)} &\sim \text{Beta}(1 + 9, 1 + 11)
\end{align*}
- Draw updated values.
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
c(rbeta(n = 1, shape1 = 1 + 24, shape2 = 1 +  6),
  rbeta(n = 1, shape1 = 1 +  9, shape2 = 1 + 11)) %>% round(3)
#+END_SRC
\normalsize
- We now have updated parameter draws.
  - $\theta_{0}^{(2)} := 0.677$
  - $\theta_{1}^{(2)} := 0.483$


- In the limit, the draws for the missing data (I-Step) and the parameters (P-Step) are from the joint posterior /distribution/ of the missing data and the parameters. cite:littleStatisticalAnalysisMissing2002
- Note that this algorithm does not converge to a point unlike the EM algorithm.

** Automated Version
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
ip_step <- function(theta, a, b) {
  X <- c(5,9,8,4,7)
  imp_coin <- bind_rows(rel_dbinom(X[1], theta),
                        rel_dbinom(X[2], theta),
                        rel_dbinom(X[3], theta),
                        rel_dbinom(X[4], theta),
                        rel_dbinom(X[5], theta)) %>%
    mutate(Coin = rbinom(n = 5, size = 1,
                         prob = `Prob. Coin B`),
           X = X,
           `Heads Coin A` = X * (1 - Coin),
           `Heads Coin B` = X * Coin) %>%
    select(Coin, `Prob. Coin A`, `Prob. Coin B`,
           X, `Heads Coin A`, `Heads Coin B`)
  imp_coin <- bind_cols(tibble(Index = c(as.character(1:5), "Sum")),
                        bind_rows(imp_coin, colSums(imp_coin)))

  Heads_A <- imp_coin$`Heads Coin A`[6]
  Tails_A <- (5 - imp_coin$Coin[6]) * 10 - Heads_A
  Heads_B <- imp_coin$`Heads Coin B`[6]
  Tails_B <- imp_coin$Coin[6] * 10 - Heads_B
  theta_post_draws <- c(rbeta(n = 1, shape1 = a[1] + Heads_A, shape2 = b[1] + Tails_A),
                        rbeta(n = 1, shape1 = a[1] + Heads_B, shape2 = b[2] + Tails_B))

  list(I = imp_coin, P = theta_post_draws)
}

ip_iter <- function(theta, a = c(1,1), b = c(1,1), iter = 10) {
  thetas <- data.frame(theta0 = c(theta[1], rep(as.numeric(NA), iter)),
                       theta1 = c(theta[2], rep(as.numeric(NA), iter)))
  for (i in seq_len(iter)) {
    thetas[i+1,] <- ip_step(as.numeric(thetas[i,]), a, b)$P
  }
  return(as.tibble(thetas))
}
#+END_SRC
\normalsize

** Visual Representation of Initial Iterations
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- The algorithm does not converge to a point.
- Sampling is performed proportional to the posterior density.
- More samples are obtained from parameter values that are more likely.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports none
set.seed(737265171)
ip_data <- ip_iter(theta = c(0.6, 0.5), a = c(1,1), b = c(1, 1), iter = 10^4)
#+END_SRC
#+HEADER: :width 5 :height 5
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/ip_figure1.pdf :exports results
max_iter <- 49
ip_data[seq_len(max_iter + 1),] %>%
  mutate(r = seq_len(n()) - 1) %>%
  ggplot(mapping = aes(x = theta0, y = theta1)) +
  geom_path(size = 0.1) +
  geom_text(mapping = aes(label = r), size = 3) +
  geom_point(data = ip_data[c(1, max_iter + 1),],
             shape = 1, size = 5) +
  scale_x_continuous(limits = 0:1) +
  scale_y_continuous(limits = 0:1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5),
        legend.key = element_blank(),
        plot.title = element_text(hjust = 0.5),
        strip.background = element_blank())
#+END_SRC

#+ATTR_LATEX: :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/ip_figure1.pdf]]
\normalsize

** Visual Representation of Posterior Samples
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- 10^{4} posterior samples were obtained.
- The first 10% of posterior samples were discarded to reduce the influence of the initialization values.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 5 :height 5
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/ip_figure2.pdf :exports results
ip_data[seq(round(nrow(ip_data) * 0.1), nrow(ip_data)),] %>%
  ggplot(mapping = aes(x = theta0, y = theta1)) +
  geom_point(size = 0.1, alpha = 0.5) +
  scale_x_continuous(limits = 0:1) +
  scale_y_continuous(limits = 0:1) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5),
        legend.key = element_blank(),
        plot.title = element_text(hjust = 0.5),
        strip.background = element_blank())
#+END_SRC

#+ATTR_LATEX: :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/ip_figure2.pdf]]
\normalsize

** Visual Representation of Posterior Density
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- The first 10% of posterior samples were discarded to reduce the influence of the initialization values.
- Similarly to the EM results with multiple initialization values, the posterior exhibits bimodality.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 7 :height 7
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/ip_figure3.pdf :exports results
with(ip_data[seq(round(nrow(ip_data) * 0.1), nrow(ip_data)),],
     persp(MASS::kde2d(theta0, theta1, n = 100),
           main = "Joint Posterior",
           xlab = "theta0", ylab = "theta1", zlab = "kernel density estimate"))
#+END_SRC

#+ATTR_LATEX: :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/em_figure3.pdf]]
\normalsize

* Stan
** Title Stan
   :PROPERTIES:
   :BEAMER_ENV: fullframe
   :END:
#+BEGIN_CENTER
\resizebox{\linewidth}{!}{Sampling Using Stan}
#+END_CENTER
** Stan: Hamiltonian Monte Carlo
- Traditional Bayesian posterior sampling software, such as WinBUGS cite:lunnWinBUGSBayesianModelling2000 and JAGS cite:plummerJAGSProgramAnalysis2003, are Gibbs samplers.
- Gibbs sampling in this incomplete-data setting implements the data augmentation method.
- Stan cite:carpenterStanProbabilisticProgramming2017 is a modern Bayesian posterior sampler, which uses more efficient Hamiltonian Monte Carlo (HMC) cite:betancourtConceptualIntroductionHamiltonian2017.
- However, HMC cannot handle discrete parameters, so the latent state have to be integrated (summed) out of the posterior (marginalized posterior).

** Marginalized Posterior Derivation
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- We are interested in the posterior distribution of the parameters given the observed data only.
\begin{align*}
  &~~~\text{Introduce latent state}\\
  p(\btheta | \bX)
  &= \sum_{\bz} p(\btheta, \bz | \bX)\\
  &= \sum^{1}_{z_{1}=0}\dots\sum^{1}_{z_{5}=0} p(\btheta, z_{1},\dots,z_{5} | \bX_{1},\dots,\bX_{5})\\
  &~~~\text{Bayes rule}\\
  &\propto \sum^{1}_{z_{1}=0}\dots\sum^{1}_{z_{5}=0} p(\btheta, z_{1},\dots,z_{5}, \bX_{1},\dots,\bX_{5})\\
  &~~~\text{iid given parameter}\\
  &= \sum^{1}_{z_{1}=0}\dots\sum^{1}_{z_{5}=0} \prod^{5}_{i=1}p(X_{i} | z_{i},\btheta) p(z_{i}) p(\btheta)\\
  % https://math.stackexchange.com/questions/705945/how-to-interchange-a-sum-and-a-product
  &= \prod^{5}_{i=1} \sum^{1}_{z_{i}=0} p(X_{i} | z_{i},\btheta) p(z_{i}) p(\btheta)\\
  &= p(\btheta) \prod^{5}_{i=1} \sum^{1}_{z_{i}=0} p(X_{i} | z_{i},\btheta) p(z_{i})\\
  &= p(\btheta) \prod^{5}_{i=1} \sum^{1}_{z_{i}=0} p(X_{i} | z_{i}, \btheta) (0.5)\\
  &\propto p(\btheta) \prod^{5}_{i=1} \sum^{1}_{z_{i}=0} p(X_{i} | z_{i}, \btheta)
\end{align*}

** Stan Implementation
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
- The marginalized posterior expression can be implemented as follows in the Stan language.
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports both
stan_code <- readr::read_file("./coin.stan")
cat(stan_code)
#+END_SRC
\normalsize
\scriptsize
#+BEGIN_SRC R :session *R-org* :results output :exports none
suppressMessages(library(rstan))
coin_sample <- rstan::stan(model_code = stan_code,
                           data = list(a = c(1,1),
                                       b = c(1,1),
                                       N = 5,
                                       X = c(5,9,8,4,7)),
                           iter = 10^4,
                           chains = 12)
#+END_SRC
\normalsize

** Stan Posterior Samples
*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
- The posterior distribution is essentially the same as the data augmentation version.
- The same bimodality issue persists.

*** @@latex:@@                                                        :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.50
    :END:
\scriptsize
#+HEADER: :width 7 :height 7
#+BEGIN_SRC R :session *R-org* :results output graphics :file ./source/figure_stan.pdf :exports results
tidybayes::tidy_draws(coin_sample) %>%
  select(`theta[1]`, `theta[2]`) %>%
  with(.,
       persp(MASS::kde2d(`theta[1]`, `theta[2]`, n = 100),
             xlim = c(0,1), ylim = c(0,1),
             main = "Joint Posterior (Stan)",
             xlab = "theta0", ylab = "theta1", zlab = "kernel density estimate"))
#+END_SRC

#+ATTR_LATEX: :height \textheight :width \textwidth :options page=1,keepaspectratio :center t
#+RESULTS:
[[file:./source/figure_stan.pdf]]
\normalsize


* Appendix
\appendix
** Bibliography
   :PROPERTIES:
   :BEAMER_opt: allowframebreaks,label=,t
   :END:
\tiny
# To remove "References" section header
\renewcommand{\section}[2]{}
# Following lines must be left-aligned without preceding spaces.
bibliographystyle:apalike
bibliography:~/.emacs.d/misc/zotero.bib
